{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd71534",
   "metadata": {},
   "source": [
    "# Search Engine for Medium Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf54fd",
   "metadata": {},
   "source": [
    "- **Tokenization**\n",
    "- **Word Co Occurence Matrix**\n",
    "- **Continouous Bag of Words (CBoW)**\n",
    "- **Word2Vec**\n",
    "- **Search Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "697a1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "import nltk, re, string, contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d202a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>author</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/ensemble-method...</td>\n",
       "      <td>Ensemble methods: bagging, boosting and stacking</td>\n",
       "      <td>Understanding the key concepts of ensemble lea...</td>\n",
       "      <td>Joseph Rocca</td>\n",
       "      <td>20</td>\n",
       "      <td>This post was co-written with Baptiste Rocca.\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/understanding-a...</td>\n",
       "      <td>Understanding AUC - ROC Curve</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>Sarang Narkhede</td>\n",
       "      <td>5</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-work-wit...</td>\n",
       "      <td>How to work with object detection datasets in ...</td>\n",
       "      <td>A comprehensive guide to defining, loading, ex...</td>\n",
       "      <td>Eric Hofesmann</td>\n",
       "      <td>10</td>\n",
       "      <td>Microsoft's Common Objects in Context dataset ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/11-dimensionali...</td>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>Reduce the size of your dataset while keeping ...</td>\n",
       "      <td>Rukshan Pramoditha</td>\n",
       "      <td>16</td>\n",
       "      <td>In both Statistics and Machine Learning, the n...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/the-time-series...</td>\n",
       "      <td>The Time Series Transformer</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>Theodoros Ntakouris</td>\n",
       "      <td>6</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://towardsdatascience.com/ensemble-method...   \n",
       "1  https://towardsdatascience.com/understanding-a...   \n",
       "2  https://towardsdatascience.com/how-to-work-wit...   \n",
       "3  https://towardsdatascience.com/11-dimensionali...   \n",
       "4  https://towardsdatascience.com/the-time-series...   \n",
       "\n",
       "                                               title  \\\n",
       "0   Ensemble methods: bagging, boosting and stacking   \n",
       "1                      Understanding AUC - ROC Curve   \n",
       "2  How to work with object detection datasets in ...   \n",
       "3  11 Dimensionality reduction techniques you sho...   \n",
       "4                        The Time Series Transformer   \n",
       "\n",
       "                                           sub_title               author  \\\n",
       "0  Understanding the key concepts of ensemble lea...         Joseph Rocca   \n",
       "1  In Machine Learning, performance measurement i...      Sarang Narkhede   \n",
       "2  A comprehensive guide to defining, loading, ex...       Eric Hofesmann   \n",
       "3  Reduce the size of your dataset while keeping ...   Rukshan Pramoditha   \n",
       "4  Attention Is All You Need they said. Is it a m...  Theodoros Ntakouris   \n",
       "\n",
       "   reading_time                                               text  id  \n",
       "0            20  This post was co-written with Baptiste Rocca.\\...   1  \n",
       "1             5  In Machine Learning, performance measurement i...   2  \n",
       "2            10  Microsoft's Common Objects in Context dataset ...   3  \n",
       "3            16  In both Statistics and Machine Learning, the n...   4  \n",
       "4             6  Attention Is All You Need they said. Is it a m...   5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(r'F:\\Muthu_2023\\Personal\\NextStep\\NLP\\NLP\\Dataset\\medium_articles_v3.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0281add",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6026ebc",
   "metadata": {},
   "source": [
    "`As per Analysis, Article 67 contains 10000+ unique words due to the presence of names of google scholars`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3541dcf",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "79f445b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('English')\n",
    "    sent_processed = []\n",
    "    for sent in sent_tokens:\n",
    "        sent = re.sub(r'[^a-zA-Z0-9 ]',' ', contractions.fix(sent.lower()))\n",
    "        sent = re.sub(r'https://[^\\s\\n\\r]+', '', sent) #Remove links\n",
    "        sent = re.sub(r'http://[^\\s\\n\\r]+', '', sent)\n",
    "        sent = re.sub(r'[^a-zA-Z0-9 ]',' ', sent)\n",
    "        word_list = []\n",
    "        for word in sent.split():\n",
    "            if word not in stop_words and len(word.strip()) > 1 and not word.isnumeric() and not bool(re.search(r'\\d', word)) and len(word.strip()) < 20:\n",
    "                word_list.append(word)\n",
    "        if len(word_list)>0:\n",
    "            sent_processed.append(' '.join(word_list))\n",
    "    return(sent_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3bf2490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>author</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>transformed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/ensemble-method...</td>\n",
       "      <td>Ensemble methods: bagging, boosting and stacking</td>\n",
       "      <td>Understanding the key concepts of ensemble lea...</td>\n",
       "      <td>Joseph Rocca</td>\n",
       "      <td>20</td>\n",
       "      <td>This post was co-written with Baptiste Rocca.\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>[post co written baptiste rocca, unity strengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/understanding-a...</td>\n",
       "      <td>Understanding AUC - ROC Curve</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>Sarang Narkhede</td>\n",
       "      <td>5</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>2</td>\n",
       "      <td>[machine learning performance measurement esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-work-wit...</td>\n",
       "      <td>How to work with object detection datasets in ...</td>\n",
       "      <td>A comprehensive guide to defining, loading, ex...</td>\n",
       "      <td>Eric Hofesmann</td>\n",
       "      <td>10</td>\n",
       "      <td>Microsoft's Common Objects in Context dataset ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[microsoft common objects context dataset coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/11-dimensionali...</td>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>Reduce the size of your dataset while keeping ...</td>\n",
       "      <td>Rukshan Pramoditha</td>\n",
       "      <td>16</td>\n",
       "      <td>In both Statistics and Machine Learning, the n...</td>\n",
       "      <td>4</td>\n",
       "      <td>[statistics machine learning number attributes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/the-time-series...</td>\n",
       "      <td>The Time Series Transformer</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>Theodoros Ntakouris</td>\n",
       "      <td>6</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>5</td>\n",
       "      <td>[attention need said, robust convolution, hack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://towardsdatascience.com/ensemble-method...   \n",
       "1  https://towardsdatascience.com/understanding-a...   \n",
       "2  https://towardsdatascience.com/how-to-work-wit...   \n",
       "3  https://towardsdatascience.com/11-dimensionali...   \n",
       "4  https://towardsdatascience.com/the-time-series...   \n",
       "\n",
       "                                               title  \\\n",
       "0   Ensemble methods: bagging, boosting and stacking   \n",
       "1                      Understanding AUC - ROC Curve   \n",
       "2  How to work with object detection datasets in ...   \n",
       "3  11 Dimensionality reduction techniques you sho...   \n",
       "4                        The Time Series Transformer   \n",
       "\n",
       "                                           sub_title               author  \\\n",
       "0  Understanding the key concepts of ensemble lea...         Joseph Rocca   \n",
       "1  In Machine Learning, performance measurement i...      Sarang Narkhede   \n",
       "2  A comprehensive guide to defining, loading, ex...       Eric Hofesmann   \n",
       "3  Reduce the size of your dataset while keeping ...   Rukshan Pramoditha   \n",
       "4  Attention Is All You Need they said. Is it a m...  Theodoros Ntakouris   \n",
       "\n",
       "   reading_time                                               text  id  \\\n",
       "0            20  This post was co-written with Baptiste Rocca.\\...   1   \n",
       "1             5  In Machine Learning, performance measurement i...   2   \n",
       "2            10  Microsoft's Common Objects in Context dataset ...   3   \n",
       "3            16  In both Statistics and Machine Learning, the n...   4   \n",
       "4             6  Attention Is All You Need they said. Is it a m...   5   \n",
       "\n",
       "                                    transformed_text  \n",
       "0  [post co written baptiste rocca, unity strengt...  \n",
       "1  [machine learning performance measurement esse...  \n",
       "2  [microsoft common objects context dataset coco...  \n",
       "3  [statistics machine learning number attributes...  \n",
       "4  [attention need said, robust convolution, hack...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['transformed_text'] = raw_data['text'].apply(text_preprocess)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a025fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   link              208 non-null    object\n",
      " 1   title             208 non-null    object\n",
      " 2   sub_title         208 non-null    object\n",
      " 3   author            208 non-null    object\n",
      " 4   reading_time      208 non-null    int64 \n",
      " 5   text              208 non-null    object\n",
      " 6   id                208 non-null    int64 \n",
      " 7   transformed_text  208 non-null    object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 13.1+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c209e817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4634467",
   "metadata": {},
   "source": [
    "## Word Co Occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9abe2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = raw_data['transformed_text'].explode()\n",
    "voc_list = sent_list.str.split().explode().unique()\n",
    "print('Vocabulary Size: ', len(voc_list), 'No. of sentences: ', len(sent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8bb9ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for sentence in sent_list:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) not in d:\n",
    "            if (words[i+1], words[i]) not in d:\n",
    "                d[(words[i], words[i+1])] = 1\n",
    "            else:\n",
    "                d[(words[i+1], words[i])] += 1\n",
    "        else:\n",
    "            d[(words[i], words[i+1])] += 1\n",
    "            \n",
    "        if (words[i], words[i+2]) not in d:\n",
    "            if (words[i+2], words[i]) not in d:\n",
    "                d[(words[i], words[i+2])] = 1\n",
    "            else:\n",
    "                d[(words[i+2], words[i])] += 1\n",
    "        else:\n",
    "            d[(words[i], words[i+2])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "59964ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "for sentence in sent_list:\n",
    "    words = sentence.split()\n",
    "    for ind in range(len(words)):\n",
    "        pair_list = []\n",
    "        for sub_ind in range(ind - 2, ind + 3):\n",
    "            if sub_ind != ind and sub_ind >= 0 and sub_ind < len(words):\n",
    "                pair_list.append(words[sub_ind])                \n",
    "        if len(pair_list) > 0:\n",
    "            x_list.append(pair_list)\n",
    "            y_list.append([words[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0ae3b7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244928, 244928)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_list), len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "38379508",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes = voc_list, sparse_output=True) # Generates Multi label Encoding with sparse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a37fca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = mlb.fit_transform(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0864538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = mlb.fit_transform(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6e2fc636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['post', 'co', 'written', ..., 'serotonin', 'gobbled', 'critic'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "78e80cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "to_coo not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[262], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xtrain\u001b[38;5;241m.\u001b[39mto_coo()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: to_coo not found"
     ]
    }
   ],
   "source": [
    "xtrain.to_coo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0b2bd",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6a1c3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b783c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9918b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 10\n",
    "voc_size = len(voc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b413e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(voc_size,), sparse=True))\n",
    "model.add(Dense(vec_size, activation='relu'))\n",
    "model.add(Dense(voc_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2ec337b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3236b444",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\backend.py\", line 5607, in sparse_categorical_crossentropy\n        target = tf.convert_to_tensor(target)\n\n    TypeError: Failed to convert elements of SparseTensor(indices=Tensor(\"DeserializeSparse_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse_1:1\", shape=(None,), dtype=int32), dense_shape=Tensor(\"stack_1:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[276], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(csr_matrix\u001b[38;5;241m.\u001b[39msorted_indices(xtrain), csr_matrix\u001b[38;5;241m.\u001b[39msorted_indices(ytrain), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_wsur1nd.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\backend.py\", line 5607, in sparse_categorical_crossentropy\n        target = tf.convert_to_tensor(target)\n\n    TypeError: Failed to convert elements of SparseTensor(indices=Tensor(\"DeserializeSparse_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse_1:1\", shape=(None,), dtype=int32), dense_shape=Tensor(\"stack_1:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
     ]
    }
   ],
   "source": [
    "model.fit(csr_matrix.sorted_indices(xtrain), csr_matrix.sorted_indices(ytrain), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2fda0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect\n",
    "we conjure the spirits of the computer with our spells.\"\"\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7b5d3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sentences.split()\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9d1ee46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "68d648d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input must be a SparseTensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[263], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mreorder(xtrain)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:847\u001b[0m, in \u001b[0;36msparse_reorder\u001b[1;34m(sp_input, name)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse.reorder\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse.reorder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_reorder\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    811\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_endpoints(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_reorder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msparse_reorder\u001b[39m(sp_input, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    813\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reorders a `SparseTensor` into the canonical, row-major ordering.\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \n\u001b[0;32m    815\u001b[0m \u001b[38;5;124;03m  Note that by convention, all sparse ops preserve the canonical ordering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    TypeError: If `sp_input` is not a `SparseTensor`.\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 847\u001b[0m   sp_input \u001b[38;5;241m=\u001b[39m _convert_to_sparse_tensor(sp_input)\n\u001b[0;32m    849\u001b[0m   reordered_ind, reordered_val \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    850\u001b[0m       gen_sparse_ops\u001b[38;5;241m.\u001b[39msparse_reorder(\n\u001b[0;32m    851\u001b[0m           sp_input\u001b[38;5;241m.\u001b[39mindices, sp_input\u001b[38;5;241m.\u001b[39mvalues, sp_input\u001b[38;5;241m.\u001b[39mdense_shape, name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sp_input\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mis_fully_defined():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:67\u001b[0m, in \u001b[0;36m_convert_to_sparse_tensor\u001b[1;34m(sp_input)\u001b[0m\n\u001b[0;32m     65\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor\u001b[38;5;241m.\u001b[39mfrom_value(sp_input)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sp_input, sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor):\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a SparseTensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sp_input\n",
      "\u001b[1;31mTypeError\u001b[0m: Input must be a SparseTensor."
     ]
    }
   ],
   "source": [
    "tf.sparse.reorder(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "621a4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8bf5141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ = csr_matrix.sorted_indices(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f4df22c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<244928x19511 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 818906 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bdbb0",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "300d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of list\n",
    "sentence_list = []\n",
    "for sent in sent_list:\n",
    "    word_list = []\n",
    "    for word in sent.split():\n",
    "        word_list.append(word)\n",
    "    sentence_list.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "be32798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentence_list, window=2, vector_size=100, sg=0, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "9e0dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sentence_list, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "6e0abc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2414913, 2458680)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentence_list, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "e041eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vending', 0.9781566262245178),\n",
       " ('washing', 0.9589352011680603),\n",
       " ('translation', 0.9211783409118652),\n",
       " ('slot', 0.9177753925323486),\n",
       " ('elliptical', 0.9150514602661133),\n",
       " ('dug', 0.9093351364135742),\n",
       " ('clawed', 0.9092390537261963),\n",
       " ('cpap', 0.9047977328300476),\n",
       " ('repair', 0.9045436382293701),\n",
       " ('mastercard', 0.903074324131012)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"learning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3d652e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.24759984e-01,  8.64426970e-01,  1.09302449e+00,  7.42402256e-01,\n",
       "       -2.15008545e+00, -1.08801043e+00,  6.40723467e-01,  4.89838272e-01,\n",
       "       -6.58541799e-01, -1.69573104e+00, -8.41604590e-01, -1.49746466e+00,\n",
       "        4.63539332e-01,  2.73074299e-01,  1.28581202e+00, -1.45257592e+00,\n",
       "        8.89300048e-01, -1.25001347e+00, -1.33422530e+00, -1.85545683e+00,\n",
       "        2.47237757e-01,  6.67589486e-01,  4.34866488e-01, -3.99671756e-02,\n",
       "       -1.36693752e+00,  4.69376028e-01, -1.59108460e+00,  5.23888886e-01,\n",
       "       -5.02065718e-01,  8.63543868e-01,  5.00534594e-01, -7.08035588e-01,\n",
       "        1.96976960e-01, -2.09720635e+00, -5.82256317e-01,  9.67792749e-01,\n",
       "        9.61209610e-02, -7.44150400e-01, -1.66353607e+00, -6.20042861e-01,\n",
       "       -1.34150818e-01,  8.35871339e-01,  6.93104386e-01,  7.01354086e-01,\n",
       "        7.66724423e-02, -1.66193402e+00, -1.63881516e+00, -1.37015915e+00,\n",
       "        1.44539297e+00,  1.12358975e+00, -1.22417712e+00, -9.21171904e-01,\n",
       "        1.47957332e-03, -7.25116432e-01, -9.85961080e-01,  2.25494051e+00,\n",
       "       -2.42048100e-01,  4.54903066e-01, -8.38826358e-01,  1.84914732e+00,\n",
       "        2.47091018e-02, -7.38892496e-01, -3.50697994e-01,  5.26187956e-01,\n",
       "       -8.81755710e-01,  1.11566973e+00, -1.64968133e+00,  1.59621012e+00,\n",
       "       -3.70405346e-01,  5.85386336e-01,  3.24120224e-01,  7.62187898e-01,\n",
       "        2.45552588e+00, -3.49172235e-01,  1.19474101e+00,  1.57628119e+00,\n",
       "        8.52351785e-01, -6.12233698e-01, -6.35193408e-01, -6.36387110e-01,\n",
       "       -2.22557950e+00,  5.39078832e-01, -1.38080204e+00,  9.01765168e-01,\n",
       "       -8.80955100e-01, -1.15433745e-01,  6.01367831e-01,  2.51162022e-01,\n",
       "        1.82061577e+00, -2.53645107e-02,  1.38086975e+00,  1.56678502e-02,\n",
       "       -7.14561820e-01,  3.08069736e-01, -1.63088858e-01,  5.34212351e-01,\n",
       "       -1.05793440e+00, -7.43434310e-01,  9.00350571e-01,  3.18436950e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f1c055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Centroid\n",
    "raw_data['Centroid_cbow'] = [[0.0] * 100] * raw_data.shape[0]\n",
    "for index in range(len(raw_data)):\n",
    "    centroid = np.array([0.0] * 100)\n",
    "    article = raw_data['transformed_text'].iloc[index]\n",
    "    for sent in article:\n",
    "        for word in sent.split():\n",
    "            try:\n",
    "                centroid = np.add(centroid, model.wv[word])\n",
    "            except:\n",
    "                continue\n",
    "    raw_data['Centroid_cbow'].iloc[index] = centroid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3510959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(raw_data)):\n",
    "    cent_article = raw_data['Centroid_cbow'].iloc[index]\n",
    "    cos_sim = 0\n",
    "    for word in query:\n",
    "        cos_sim += (cosine_similarity(model.wv[word], cent_article))\n",
    "    cos_sim_list(row[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
