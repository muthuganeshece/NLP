{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd71534",
   "metadata": {},
   "source": [
    "# Search Engine for Medium Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf54fd",
   "metadata": {},
   "source": [
    "`In this project, a basic Search Engine was created using the TF-IDF method to rank documents based on relevance. The project involves the following steps:`\n",
    "\n",
    "- **Text Preprocessing**\n",
    "- **Word Co Occurence Matrix**\n",
    "- **Continouous Bag of Words (CBoW)**\n",
    "- **Skipgram**\n",
    "- **Word2Vec**\n",
    "- **TF IDF weighted Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697a1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "\n",
    "import nltk, re, string, contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24e62e0-59c7-4ce2-aa78-71edd658448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Muthukumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d202a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>author</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/ensemble-method...</td>\n",
       "      <td>Ensemble methods: bagging, boosting and stacking</td>\n",
       "      <td>Understanding the key concepts of ensemble lea...</td>\n",
       "      <td>Joseph Rocca</td>\n",
       "      <td>20</td>\n",
       "      <td>This post was co-written with Baptiste Rocca.\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/understanding-a...</td>\n",
       "      <td>Understanding AUC - ROC Curve</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>Sarang Narkhede</td>\n",
       "      <td>5</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-work-wit...</td>\n",
       "      <td>How to work with object detection datasets in ...</td>\n",
       "      <td>A comprehensive guide to defining, loading, ex...</td>\n",
       "      <td>Eric Hofesmann</td>\n",
       "      <td>10</td>\n",
       "      <td>Microsoft's Common Objects in Context dataset ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/11-dimensionali...</td>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>Reduce the size of your dataset while keeping ...</td>\n",
       "      <td>Rukshan Pramoditha</td>\n",
       "      <td>16</td>\n",
       "      <td>In both Statistics and Machine Learning, the n...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/the-time-series...</td>\n",
       "      <td>The Time Series Transformer</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>Theodoros Ntakouris</td>\n",
       "      <td>6</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://towardsdatascience.com/ensemble-method...   \n",
       "1  https://towardsdatascience.com/understanding-a...   \n",
       "2  https://towardsdatascience.com/how-to-work-wit...   \n",
       "3  https://towardsdatascience.com/11-dimensionali...   \n",
       "4  https://towardsdatascience.com/the-time-series...   \n",
       "\n",
       "                                               title  \\\n",
       "0   Ensemble methods: bagging, boosting and stacking   \n",
       "1                      Understanding AUC - ROC Curve   \n",
       "2  How to work with object detection datasets in ...   \n",
       "3  11 Dimensionality reduction techniques you sho...   \n",
       "4                        The Time Series Transformer   \n",
       "\n",
       "                                           sub_title               author  \\\n",
       "0  Understanding the key concepts of ensemble lea...         Joseph Rocca   \n",
       "1  In Machine Learning, performance measurement i...      Sarang Narkhede   \n",
       "2  A comprehensive guide to defining, loading, ex...       Eric Hofesmann   \n",
       "3  Reduce the size of your dataset while keeping ...   Rukshan Pramoditha   \n",
       "4  Attention Is All You Need they said. Is it a m...  Theodoros Ntakouris   \n",
       "\n",
       "   reading_time                                               text  id  \n",
       "0            20  This post was co-written with Baptiste Rocca.\\...   1  \n",
       "1             5  In Machine Learning, performance measurement i...   2  \n",
       "2            10  Microsoft's Common Objects in Context dataset ...   3  \n",
       "3            16  In both Statistics and Machine Learning, the n...   4  \n",
       "4             6  Attention Is All You Need they said. Is it a m...   5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(r'F:\\Muthu_2023\\Personal\\NextStep\\NLP\\NLP\\Dataset\\medium_articles_v3.csv')\n",
    "# raw_data = pd.read_csv(r'E:\\Nextstep\\NLP\\Dataset\\medium_articles_v3.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0281add",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6026ebc",
   "metadata": {},
   "source": [
    "`As per Analysis, Article 67 contains 10000+ unique words due to the presence of names of google scholars. Hence dropped`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3541dcf",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5de4e-957b-44cf-b72b-1f2672b40076",
   "metadata": {},
   "source": [
    "- `Sentence Tokenization`\n",
    "- `Text cleaning: Links, Numbers, AlphaNumeric words, concatenated words and Stopwords removal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f445b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('English')\n",
    "    sent_processed = []\n",
    "    for sent in sent_tokens:\n",
    "        sent = re.sub(r'[^a-zA-Z0-9 ]',' ', contractions.fix(sent.lower()))\n",
    "        sent = re.sub(r'https://[^\\s\\n\\r]+', '', sent) #Remove links\n",
    "        sent = re.sub(r'http://[^\\s\\n\\r]+', '', sent)\n",
    "        sent = re.sub(r'[^a-zA-Z0-9 ]',' ', sent)\n",
    "        word_list = []\n",
    "        for word in sent.split():\n",
    "            if word not in stop_words and len(word.strip()) > 1 and not word.isnumeric() and not bool(re.search(r'\\d', word)) and len(word.strip()) < 20:\n",
    "                word_list.append(word)\n",
    "        if len(word_list)>0:\n",
    "            sent_processed.append(' '.join(word_list))\n",
    "    return(sent_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf2490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>author</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>transformed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://towardsdatascience.com/ensemble-method...</td>\n",
       "      <td>Ensemble methods: bagging, boosting and stacking</td>\n",
       "      <td>Understanding the key concepts of ensemble lea...</td>\n",
       "      <td>Joseph Rocca</td>\n",
       "      <td>20</td>\n",
       "      <td>This post was co-written with Baptiste Rocca.\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>[post co written baptiste rocca, unity strengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/understanding-a...</td>\n",
       "      <td>Understanding AUC - ROC Curve</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>Sarang Narkhede</td>\n",
       "      <td>5</td>\n",
       "      <td>In Machine Learning, performance measurement i...</td>\n",
       "      <td>2</td>\n",
       "      <td>[machine learning performance measurement esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/how-to-work-wit...</td>\n",
       "      <td>How to work with object detection datasets in ...</td>\n",
       "      <td>A comprehensive guide to defining, loading, ex...</td>\n",
       "      <td>Eric Hofesmann</td>\n",
       "      <td>10</td>\n",
       "      <td>Microsoft's Common Objects in Context dataset ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[microsoft common objects context dataset coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/11-dimensionali...</td>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>Reduce the size of your dataset while keeping ...</td>\n",
       "      <td>Rukshan Pramoditha</td>\n",
       "      <td>16</td>\n",
       "      <td>In both Statistics and Machine Learning, the n...</td>\n",
       "      <td>4</td>\n",
       "      <td>[statistics machine learning number attributes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/the-time-series...</td>\n",
       "      <td>The Time Series Transformer</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>Theodoros Ntakouris</td>\n",
       "      <td>6</td>\n",
       "      <td>Attention Is All You Need they said. Is it a m...</td>\n",
       "      <td>5</td>\n",
       "      <td>[attention need said, robust convolution, hack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://towardsdatascience.com/ensemble-method...   \n",
       "1  https://towardsdatascience.com/understanding-a...   \n",
       "2  https://towardsdatascience.com/how-to-work-wit...   \n",
       "3  https://towardsdatascience.com/11-dimensionali...   \n",
       "4  https://towardsdatascience.com/the-time-series...   \n",
       "\n",
       "                                               title  \\\n",
       "0   Ensemble methods: bagging, boosting and stacking   \n",
       "1                      Understanding AUC - ROC Curve   \n",
       "2  How to work with object detection datasets in ...   \n",
       "3  11 Dimensionality reduction techniques you sho...   \n",
       "4                        The Time Series Transformer   \n",
       "\n",
       "                                           sub_title               author  \\\n",
       "0  Understanding the key concepts of ensemble lea...         Joseph Rocca   \n",
       "1  In Machine Learning, performance measurement i...      Sarang Narkhede   \n",
       "2  A comprehensive guide to defining, loading, ex...       Eric Hofesmann   \n",
       "3  Reduce the size of your dataset while keeping ...   Rukshan Pramoditha   \n",
       "4  Attention Is All You Need they said. Is it a m...  Theodoros Ntakouris   \n",
       "\n",
       "   reading_time                                               text  id  \\\n",
       "0            20  This post was co-written with Baptiste Rocca.\\...   1   \n",
       "1             5  In Machine Learning, performance measurement i...   2   \n",
       "2            10  Microsoft's Common Objects in Context dataset ...   3   \n",
       "3            16  In both Statistics and Machine Learning, the n...   4   \n",
       "4             6  Attention Is All You Need they said. Is it a m...   5   \n",
       "\n",
       "                                    transformed_text  \n",
       "0  [post co written baptiste rocca, unity strengt...  \n",
       "1  [machine learning performance measurement esse...  \n",
       "2  [microsoft common objects context dataset coco...  \n",
       "3  [statistics machine learning number attributes...  \n",
       "4  [attention need said, robust convolution, hack...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['transformed_text'] = raw_data['text'].apply(text_preprocess)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a025fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 207 entries, 0 to 207\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   link              207 non-null    object\n",
      " 1   title             207 non-null    object\n",
      " 2   sub_title         207 non-null    object\n",
      " 3   author            207 non-null    object\n",
      " 4   reading_time      207 non-null    int64 \n",
      " 5   text              207 non-null    object\n",
      " 6   id                207 non-null    int64 \n",
      " 7   transformed_text  207 non-null    object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 14.6+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c209e817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4634467",
   "metadata": {},
   "source": [
    "## Word Co Occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6994bf2-a3f3-431e-8dcd-f11c79e83c59",
   "metadata": {},
   "source": [
    "- `Prepare list of vocabulary`\n",
    "- `Prepared word co occurence matrix: sparse`\n",
    "- `Prepare training data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abe2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  19511 No. of sentences:  26534\n"
     ]
    }
   ],
   "source": [
    "sent_list = raw_data['transformed_text'].explode()\n",
    "voc_list = sent_list.str.split().explode().unique()\n",
    "print('Vocabulary Size: ', len(voc_list), 'No. of sentences: ', len(sent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb9ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for sentence in sent_list:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) not in d:\n",
    "            if (words[i+1], words[i]) not in d:\n",
    "                d[(words[i], words[i+1])] = 1\n",
    "            else:\n",
    "                d[(words[i+1], words[i])] += 1\n",
    "        else:\n",
    "            d[(words[i], words[i+1])] += 1\n",
    "            \n",
    "        if (words[i], words[i+2]) not in d:\n",
    "            if (words[i+2], words[i]) not in d:\n",
    "                d[(words[i], words[i+2])] = 1\n",
    "            else:\n",
    "                d[(words[i+2], words[i])] += 1\n",
    "        else:\n",
    "            d[(words[i], words[i+2])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59964ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "for sentence in sent_list:\n",
    "    words = sentence.split()\n",
    "    for ind in range(len(words)):\n",
    "        pair_list = []\n",
    "        for sub_ind in range(ind - 2, ind + 3):\n",
    "            if sub_ind != ind and sub_ind >= 0 and sub_ind < len(words):\n",
    "                pair_list.append(words[sub_ind])                \n",
    "        if len(pair_list) > 0:\n",
    "            x_list.append(pair_list)\n",
    "            y_list.append([words[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae3b7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244928, 244928)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_list), len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38379508",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes = voc_list, sparse_output=True) # Generates Multi label Encoding with sparse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a37fca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = mlb.fit_transform(x_list)\n",
    "ytrain = mlb.fit_transform(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e2fc636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['post', 'co', 'written', ..., 'serotonin', 'gobbled', 'critic'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0b2bd",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a1c3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9918b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 10\n",
    "voc_size = len(voc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b413e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(voc_size,), sparse=True))\n",
    "model.add(Dense(vec_size, activation='relu'))\n",
    "model.add(Dense(voc_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ec337b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d262907b-b1ac-410d-9f55-0476064bbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eedd0439-1b3c-4a3a-9ea9-1f5a35d9ba18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(indices=tf.Tensor(\n",
       "[[     0      0]\n",
       " [     1      1]\n",
       " [     2      2]\n",
       " ...\n",
       " [244925     62]\n",
       " [244926   1479]\n",
       " [244927     11]], shape=(244928, 2), dtype=int64), values=tf.Tensor([1 1 1 ... 1 1 1], shape=(244928,), dtype=int32), dense_shape=tf.Tensor([244928  19511], shape=(2,), dtype=int64))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66940668-c580-480a-ba9f-0151b9f5c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(xtrain)), tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(ytrain)), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcc938-e329-4304-a841-73c49a2fca16",
   "metadata": {},
   "source": [
    "`Problem with sparse matrix in the network: Needs further investigation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bdbb0",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1571eae-fe7b-4bf0-95f3-da858c7bdbdf",
   "metadata": {},
   "source": [
    "- `Continuous Bag of Words implementation (CBoW) using word2vec`\n",
    "- `Skipgram implementation using word2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1e1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of list\n",
    "sentence_list = []\n",
    "for sent in sent_list:\n",
    "    word_list = []\n",
    "    for word in sent.split():\n",
    "        word_list.append(word)\n",
    "    sentence_list.append(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be619a-3660-42bf-9129-27a52cafeb63",
   "metadata": {},
   "source": [
    "## CBoW using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be32798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentence_list, window=2, vector_size=100, sg=0, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "9e0dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.build_vocab(sentence_list, progress_per=10000)\n",
    "# model.train(sentence_list, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e041eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vending', 0.9217960834503174),\n",
       " ('translation', 0.8929186463356018),\n",
       " ('washing', 0.8903473615646362),\n",
       " ('repair', 0.8423986434936523),\n",
       " ('earliest', 0.8414889574050903),\n",
       " ('slot', 0.8408620953559875),\n",
       " ('envelopes', 0.8398900032043457),\n",
       " ('capsules', 0.8373095989227295),\n",
       " ('volunteered', 0.834150493144989),\n",
       " ('mastercard', 0.8338434100151062)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"learning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79598278-1650-4d81-815b-54c10086ef68",
   "metadata": {},
   "source": [
    "`Extract similar words for the given word from the model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d652e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0237862 , -0.38215688, -0.05556021,  0.39030933,  0.26835766,\n",
       "       -0.9685569 ,  1.1269547 ,  0.52827674, -0.67589307, -0.10392741,\n",
       "        0.49126264, -0.1965297 , -0.37685892,  1.7085149 , -0.33567706,\n",
       "       -0.59292144,  1.4235001 ,  0.5721935 , -0.15958312, -2.5173852 ,\n",
       "        0.8205378 , -0.2597604 ,  1.7086773 , -0.3014652 , -0.41533405,\n",
       "        0.42606047, -0.4873449 , -0.50534207, -0.03519989, -0.40178236,\n",
       "        1.7446532 ,  1.5876681 , -0.11230429,  0.49677545, -1.1850039 ,\n",
       "        0.61231095, -1.5197169 , -1.6955729 , -1.3053275 , -2.140613  ,\n",
       "        0.86405545, -1.0840689 ,  0.9455743 , -1.2610888 ,  0.11255067,\n",
       "        0.91128683, -1.0392733 , -1.7087599 , -0.18029949,  0.3970201 ,\n",
       "        0.02042915, -1.580043  , -0.9794143 ,  0.09938424, -1.7822664 ,\n",
       "       -0.83774525, -0.60228837, -1.6656373 , -1.8647523 , -1.2973278 ,\n",
       "       -0.09623057, -0.44136506,  0.49341574,  0.07339007, -0.8351086 ,\n",
       "        1.2139884 , -0.3670534 , -0.42476758, -1.1924977 , -0.4767573 ,\n",
       "       -0.07439489,  0.9740269 , -0.57564634,  0.6969304 ,  1.2859291 ,\n",
       "       -1.0374676 ,  0.67612004,  0.67106307, -0.14112842,  1.2308666 ,\n",
       "        0.9632528 , -0.19892046, -1.4245051 ,  0.6007827 , -0.14726451,\n",
       "       -1.2293389 ,  1.8038964 , -0.00761095, -0.33572933,  0.3605267 ,\n",
       "        0.4664437 , -0.05839916, -0.44769967, -0.32844216,  0.505694  ,\n",
       "        0.48499405,  1.2959687 , -1.329115  ,  0.5853634 ,  0.97204596],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a8159-2edc-4588-8170-9f725ecbfe1d",
   "metadata": {},
   "source": [
    "`Extract word vector for the given word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1c055a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthukumar\\AppData\\Local\\Temp\\ipykernel_28544\\728846989.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data['Centroid_cbow'].iloc[index] = centroid.tolist()\n"
     ]
    }
   ],
   "source": [
    "# Find Centroid\n",
    "raw_data['Centroid_cbow'] = [[0.0] * 100] * raw_data.shape[0]\n",
    "for index in range(len(raw_data)):\n",
    "    centroid = np.array([0.0] * 100)\n",
    "    article = raw_data['transformed_text'].iloc[index]\n",
    "    for sent in article:\n",
    "        for word in sent.split():\n",
    "            try:\n",
    "                centroid = np.add(centroid, model.wv[word])\n",
    "            except:\n",
    "                continue\n",
    "    raw_data['Centroid_cbow'].iloc[index] = centroid.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dbc4d-9fbf-4492-9057-c72e90de4091",
   "metadata": {},
   "source": [
    "`Generates centroid of the word vectors for each article`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3510959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_article(query, df, model, col_name):\n",
    "    cos_sim_list = []\n",
    "    for index in range(len(df)):\n",
    "        cent_article = np.array(df[col_name].iloc[index]).reshape(1,-1)\n",
    "        cos_sim = 0\n",
    "        for word in query.split():\n",
    "            try:\n",
    "                temp_cent = np.array(model.wv[word]).reshape(1,-1)\n",
    "                cos_sim += (cosine_similarity(temp_cent, cent_article))\n",
    "            except:\n",
    "                continue\n",
    "        cos_sim_list.append([df['title'].iloc[index], cos_sim[0,0]])\n",
    "    return cos_sim_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b1fcc-7d51-4b82-9384-d9e8bcf5f9ec",
   "metadata": {},
   "source": [
    "`Compares cosine similarity for each word from query with all the articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd18a1ba-bf47-4f1f-af51-48efd4787d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18    17 Clustering Algorithms Used In Data Science ...\n",
       "3     11 Dimensionality reduction techniques you sho...\n",
       "52    TRAIN A CUSTOM YOLOv4 OBJECT DETECTOR (Using G...\n",
       "19    Introduction to Genetic Algorithms  Including ...\n",
       "74    The 5 Clustering Algorithms Data Scientists Ne...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_articles = get_similar_article(\"principal component analysis\", raw_data, model, 'Centroid_cbow')\n",
    "temp_df = pd.DataFrame(similar_articles, columns=['title', 'score'])\n",
    "temp_df.sort_values('score', ascending=False)['title'].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee071ff6-9d88-465b-b931-643a14bb0b1e",
   "metadata": {},
   "source": [
    "`Returns top 5 matching articles`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fd9c4-913f-43f9-9537-ae79e03e6247",
   "metadata": {},
   "source": [
    "## Skipgram using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53647935-fd61-4510-8ee0-ee19a7140d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = Word2Vec(sentence_list, min_count=0, window=2, vector_size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8a4ec1e-93cc-4af3-961c-95042df24bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4514971 , -0.06575663, -0.04660844,  0.05198604,  0.11167115,\n",
       "       -0.33453867,  0.45436785,  0.08417453, -0.37928718,  0.00169973,\n",
       "        0.1480776 , -0.15703738, -0.14837955,  0.83874077, -0.13342898,\n",
       "       -0.38646498,  0.597749  ,  0.50272214, -0.1252687 , -0.97115844,\n",
       "        0.49803537, -0.11483087,  0.9821957 , -0.17528768, -0.22842406,\n",
       "        0.3397735 , -0.13709344, -0.5374775 , -0.12903762, -0.17602657,\n",
       "        1.0036551 ,  0.83098614, -0.01324456,  0.20545445, -0.4712755 ,\n",
       "        0.2481716 , -0.73464656, -0.727447  , -0.7408131 , -0.9686139 ,\n",
       "        0.34138075, -0.5425545 ,  0.38642862, -0.5712795 ,  0.03600386,\n",
       "        0.3683938 , -0.65368474, -0.88349015,  0.086861  ,  0.3330882 ,\n",
       "        0.09210204, -0.7773117 , -0.55614394,  0.00394889, -0.84166384,\n",
       "       -0.41658974, -0.25193766, -0.77118176, -0.9820485 , -0.56150347,\n",
       "       -0.01979044, -0.3817165 ,  0.05722544,  0.06648361, -0.3433137 ,\n",
       "        0.62319756, -0.20727728, -0.29181555, -0.5545251 , -0.11095647,\n",
       "       -0.1827145 ,  0.4046099 , -0.25723013,  0.36897883,  0.46952274,\n",
       "       -0.5169344 ,  0.25854707,  0.49446154, -0.05666538,  0.61810815,\n",
       "        0.3680099 , -0.2341235 , -0.8224055 ,  0.19652925, -0.0167584 ,\n",
       "       -0.4572373 ,  0.89801025, -0.2200404 , -0.07545637,  0.2666428 ,\n",
       "        0.08792894,  0.24304762, -0.20240892, -0.36219186,  0.10488766,\n",
       "        0.3142765 ,  0.54319715, -0.6016593 ,  0.29244438,  0.49024865],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv['learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "002d623d-1825-4914-888a-d21d9db754d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('translation', 0.8542159199714661),\n",
       " ('supervised', 0.8365312814712524),\n",
       " ('cpap', 0.8223248720169067),\n",
       " ('language', 0.8162795305252075),\n",
       " ('ai', 0.8143751621246338),\n",
       " ('geolocation', 0.8119624257087708),\n",
       " ('intelligence', 0.8092731833457947),\n",
       " ('brownlee', 0.8022918105125427),\n",
       " ('source', 0.802204430103302),\n",
       " ('python', 0.800692617893219)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(positive=['learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9609e25c-26c4-4f02-9d7c-56c21bc63b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13916\\244330531.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data['Centroid_sg'].iloc[index] = centroid_article\n"
     ]
    }
   ],
   "source": [
    "raw_data['Centroid_sg'] = [[0.0] * 100 ] * raw_data.shape[0]\n",
    "for index in range(len(raw_data)):\n",
    "    text = raw_data['transformed_text'].iloc[index]\n",
    "    centroid_article = [0.0] * 100\n",
    "    for sent in text:\n",
    "        for word in sent.split():\n",
    "            try:\n",
    "                centroid_article = np.add(centroid_article, model_sg.wv[word])\n",
    "            except:\n",
    "                continue\n",
    "    raw_data['Centroid_sg'].iloc[index] = centroid_article    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eec77-502f-44d8-bad2-d2d093dfabd2",
   "metadata": {},
   "source": [
    "`Generates centroid of the word vectors for each article`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfbebcc7-fa89-46aa-b55d-eda6a04a4208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PCA using Python (scikit-learn)</td>\n",
       "      <td>0.967104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Machine learning in finance: Why, what &amp; how</td>\n",
       "      <td>0.966168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>0.963317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>180 Data Science and Machine Learning Projects...</td>\n",
       "      <td>0.960358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Top 10 Data Science Projects for Beginners</td>\n",
       "      <td>0.959503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title     score\n",
       "49                     PCA using Python (scikit-learn)  0.967104\n",
       "131       Machine learning in finance: Why, what & how  0.966168\n",
       "3    11 Dimensionality reduction techniques you sho...  0.963317\n",
       "28   180 Data Science and Machine Learning Projects...  0.960358\n",
       "98          Top 10 Data Science Projects for Beginners  0.959503"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df_sg = pd.DataFrame(get_similar_article('pca', raw_data, model_sg, 'Centroid_sg'), columns = ['title', 'score'])\n",
    "temp_df_sg.sort_values('score', ascending=False).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf351dc-e83a-41cf-9cdb-2febebea024b",
   "metadata": {},
   "source": [
    "`Returns Top 5 matching articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ebd5dac-ac08-42d2-9570-afc946254148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('classification', 0.9591761827468872),\n",
       " ('non', 0.9544885158538818),\n",
       " ('linear', 0.9541516900062561),\n",
       " ('clustering', 0.9520301222801208),\n",
       " ('methods', 0.9456323385238647),\n",
       " ('dimensionality', 0.9435859322547913),\n",
       " ('technique', 0.9434919357299805),\n",
       " ('performance', 0.9420003890991211),\n",
       " ('ensemble', 0.9411723613739014),\n",
       " ('called', 0.9368484616279602)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(['regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74c078b2-bb4e-42ff-bb32-ebb6e941113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchword = 'regression'\n",
    "search_list = [sent for sent in sentence_list if searchword in sent]\n",
    "len(search_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4419e",
   "metadata": {},
   "source": [
    "# TFIDF weighted Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b89d8c",
   "metadata": {},
   "source": [
    "- `Calculate TF-IDF`\n",
    "- `Apply the weights while calculating word vec centroid for each article`\n",
    "- `Calculate averageTFIDF for each word and apply the weights for each word in query while calculating centroid for each query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddc74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3764360",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = [\" \".join(article) for article in raw_data['transformed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef604be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_df = pd.DataFrame(tfidf.fit_transform(processed_text).todense())\n",
    "tfidf_df.columns = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49b59866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13916\\3572736335.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data['tfidf_w2v'].iloc[index] = centroid_article\n"
     ]
    }
   ],
   "source": [
    "raw_data['tfidf_w2v'] = [[0.0] * 100 ] * raw_data.shape[0]\n",
    "for index in range(len(raw_data)):\n",
    "    text = raw_data['transformed_text'].iloc[index]\n",
    "    centroid_article = [0.0] * 100\n",
    "    for sent in text:\n",
    "        for word in sent.split():\n",
    "            try:                \n",
    "                centroid_article = np.add(centroid_article, tfidf_df.iloc[index][word] * model_sg.wv[word])\n",
    "            except:\n",
    "                continue\n",
    "    raw_data['tfidf_w2v'].iloc[index] = centroid_article   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc890330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_article1(query, df, model, col_name):\n",
    "    cos_sim_list = []\n",
    "    for index in range(len(df)):\n",
    "        cent_article = np.array(df[col_name].iloc[index]).reshape(1,-1)\n",
    "        cos_sim = 0\n",
    "        for word in query.split():\n",
    "            try:\n",
    "                temp_arr = np.array(tfidf_df.iloc[:][word])\n",
    "                tfidf_mean = temp_arr[list(np.where(temp_arr))].mean()\n",
    "                temp_cent = np.array(model.wv[word] * tfidf_mean).reshape(1,-1)\n",
    "                cos_sim += (cosine_similarity(temp_cent, cent_article))\n",
    "            except:\n",
    "                continue\n",
    "        cos_sim_list.append([df['title'].iloc[index], cos_sim[0,0]])\n",
    "    return cos_sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a779ce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 Dimensionality reduction techniques you sho...</td>\n",
       "      <td>0.979759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>PCA using Python (scikit-learn)</td>\n",
       "      <td>0.976613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensemble methods: bagging, boosting and stacking</td>\n",
       "      <td>0.970478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Understanding Contrastive Learning</td>\n",
       "      <td>0.968863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Time Series Forecasting with PyCaret Regressio...</td>\n",
       "      <td>0.966923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title     score\n",
       "3   11 Dimensionality reduction techniques you sho...  0.979759\n",
       "49                    PCA using Python (scikit-learn)  0.976613\n",
       "0    Ensemble methods: bagging, boosting and stacking  0.970478\n",
       "45                 Understanding Contrastive Learning  0.968863\n",
       "55  Time Series Forecasting with PyCaret Regressio...  0.966923"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df_tfidf = pd.DataFrame(get_similar_article1('pca', raw_data, model_sg, 'tfidf_w2v'), columns = ['title', 'score'])\n",
    "temp_df_tfidf.sort_values('score', ascending=False).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe32b56",
   "metadata": {},
   "source": [
    "# Prepared by Muthukumar G"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
